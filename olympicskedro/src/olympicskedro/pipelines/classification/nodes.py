import os
import joblib
import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold, train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, precision_recall_curve
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.combine import SMOTEENN

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
try:
    from xgboost import XGBClassifier
except Exception:
    XGBClassifier = None

try:
    from lightgbm import LGBMClassifier
except Exception:
    LGBMClassifier = None

RANDOM_STATE = 42


# =====================================================
# =============== PREPARACI√ìN DE DATOS =================
# =====================================================
def prepare_summer_data(df: pd.DataFrame):
    df = df.copy()
    
    if "Medal" not in df.columns:
        raise ValueError("No se encontr√≥ columna 'Medal' en datos")
    
    df["medal_gold"] = (df["Medal"] == "Gold").astype(int)
    
    if 'Year' in df.columns:
        df['years_since_first'] = df['Year'] - df['Year'].min()
    
    if 'Age' in df.columns:
        df['age_group'] = pd.cut(df['Age'], bins=[0, 20, 25, 30, 35, 100],
                                 labels=[0, 1, 2, 3, 4])
    
    if all(col in df.columns for col in ['NOC', 'Sport']):
        country_stats = df.groupby('NOC')['medal_gold'].agg(['mean', 'count']).rename(
            columns={'mean': 'country_win_rate', 'count': 'country_participations'})
        sport_stats = df.groupby('Sport')['medal_gold'].agg(['mean', 'count']).rename(
            columns={'mean': 'sport_win_rate', 'count': 'sport_participations'})
        df = df.merge(country_stats, on='NOC', how='left')
        df = df.merge(sport_stats, on='Sport', how='left')
    
    categorical_cols = ['Season', 'Sport', 'NOC', 'City']
    for col in categorical_cols:
        if col in df.columns and df[col].nunique() < 50:
            df[f'{col}_encoded'] = df[col].astype('category').cat.codes
    
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    cols_to_remove = ['medal_gold', 'ID', 'Year']
    features = [col for col in numeric_cols if col not in cols_to_remove and df[col].nunique() > 1]
    
    X = df[features].fillna(df[features].median())
    y = df["medal_gold"]
    
    print(f"[prepare_summer_data] Shape: {X.shape}")
    print(f"[prepare_summer_data] Class balance:\n{y.value_counts()}")
    print(f"[prepare_summer_data] Positive class ratio: {y.mean():.4f}")
    
    return train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)


# =====================================================
# =============== ENTRENAMIENTO DE MODELOS =============
# =====================================================
def train_classification_models(data):
    X_train, X_test, y_train, y_test = data
    
    print(f"[DEBUG] X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")
    print(f"[DEBUG] y_train shape: {y_train.shape}, y_test shape: {y_test.shape}")
    print(f"[DEBUG] NaN en X_train: {X_train.isna().sum().sum()}")
    
    X_train = X_train.fillna(0)
    X_test = X_test.fillna(0)
    
    if X_train.shape[1] != X_test.shape[1]:
        print(f"[WARNING] Diferente n√∫mero de features: train={X_train.shape[1]}, test={X_test.shape[1]}")
        common_cols = X_train.columns.intersection(X_test.columns)
        X_train = X_train[common_cols]
        X_test = X_test[common_cols]
    
    positive_ratio = y_train.mean()
    print(f"[train_classification_models] Positive class ratio: {positive_ratio:.4f}")
    
    if positive_ratio < 0.1:
        print("[train_classification_models] Usando SMOTEENN")
        sampling_method = SMOTEENN(
            smote=SMOTE(
                sampling_strategy=0.3,
                random_state=RANDOM_STATE,
                k_neighbors=min(3, max(1, len(y_train[y_train==1])-1))
            ),
            random_state=RANDOM_STATE
        )
    else:
        sampling_method = SMOTE(random_state=RANDOM_STATE)
    
    numeric_features = X_train.select_dtypes(include=["number"]).columns.tolist()
    preprocessor = ColumnTransformer([
        ("num", Pipeline([
            ("imputer", SimpleImputer(strategy="median")), 
            ("scaler", StandardScaler())
        ]), numeric_features),
    ], remainder="drop")
    
    # ==========================
    # 5 MODELOS DE CLASIFICACI√ìN
    # ==========================
    estimators = {
        "random_forest": RandomForestClassifier(class_weight="balanced_subsample", random_state=RANDOM_STATE),
        "logistic": LogisticRegression(max_iter=2000, class_weight="balanced", random_state=RANDOM_STATE),
        "gradient_boosting": GradientBoostingClassifier(random_state=RANDOM_STATE),
        "svm": SGDClassifier(
            loss="log_loss",
            class_weight="balanced",
            max_iter=2000,
            tol=1e-3,
            random_state=RANDOM_STATE
        )
    }
    
    # LightGBM (nuevo modelo r√°pido y potente)
    if LGBMClassifier is not None:
        estimators["lightgbm"] = LGBMClassifier(
            objective="binary",
            random_state=RANDOM_STATE,
            class_weight="balanced",
            verbosity=-1
        )
    
    # XGBoost opcional (solo si est√° disponible)
    if XGBClassifier is not None:
        scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1]) if len(y_train[y_train==1]) > 0 else 1
        estimators["xgboost"] = XGBClassifier(
            use_label_encoder=False,
            eval_metric="logloss",
            random_state=RANDOM_STATE,
            scale_pos_weight=scale_pos_weight
        )
    
    # Espacios de b√∫squeda
    param_distributions = {
        "random_forest": {
            "model__n_estimators": [200, 300, 400],
            "model__max_depth": [5, 8, 10, 12],
            "model__min_samples_split": [10, 20, 30],
            "model__min_samples_leaf": [5, 10, 15],
            "model__max_features": [0.3, 0.5, 0.7]
        },
        "logistic": {
            "model__C": [0.001, 0.01, 0.1, 1],
            "model__penalty": ['l1', 'l2'],
            "model__solver": ['liblinear']
        },
        "gradient_boosting": {
            "model__n_estimators": [100, 200, 300],
            "model__learning_rate": [0.01, 0.05, 0.1],
            "model__max_depth": [3, 4, 5, 6],
            "model__subsample": [0.7, 0.8, 0.9]
        },
        "svm": {
            "model__alpha": [1e-5, 1e-4, 1e-3, 1e-2],
            "model__penalty": ['l2', 'l1', 'elasticnet'],
            "model__l1_ratio": [0.0, 0.15, 0.3, 0.5],
            "model__learning_rate": ['optimal', 'invscaling'],
        },
        "lightgbm": {
            "model__n_estimators": [200, 300, 400],
            "model__learning_rate": [0.01, 0.05, 0.1],
            "model__num_leaves": [15, 31, 63],
            "model__max_depth": [-1, 5, 8, 10],
            "model__subsample": [0.7, 0.8, 0.9],
            "model__colsample_bytree": [0.7, 0.8, 0.9],
            "model__reg_lambda": [0.0, 0.5, 1.0]
        }
    }
    
    os.makedirs("data/06_models/classification", exist_ok=True)
    results = []
    rkf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=RANDOM_STATE)
    
    for name, estimator in estimators.items():
        print(f"\nüîß Entrenando {name} ...")
        
        try:
            pipe = ImbPipeline([
                ("pre", preprocessor),
                ("smote", sampling_method),
                ("model", estimator)
            ])
            
            rs = RandomizedSearchCV(
                estimator=pipe,
                param_distributions=param_distributions[name],
                n_iter=15,
                cv=5,
                scoring="roc_auc",
                random_state=RANDOM_STATE,
                n_jobs=-1,
                verbose=1
            )
            
            rs.fit(X_train, y_train)
            best = rs.best_estimator_
            
            y_pred_proba = best.predict_proba(X_test)[:, 1]
            precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
            best_threshold = thresholds[np.argmax(f1_scores[:-1])]
            y_pred_optimized = (y_pred_proba >= best_threshold).astype(int)
            
            acc = accuracy_score(y_test, y_pred_optimized)
            f1 = f1_score(y_test, y_pred_optimized)
            prec = precision_score(y_test, y_pred_optimized, zero_division=0)
            rec = recall_score(y_test, y_pred_optimized)
            auc = roc_auc_score(y_test, y_pred_proba)
            cv_scores = cross_val_score(best, X_train, y_train, cv=5, scoring="roc_auc", n_jobs=-1)
            
            results.append({
                "Model": name,
                "Best_Params": str(rs.best_params_),
                "Accuracy": float(acc),
                "F1_Score": float(f1),
                "Precision": float(prec),
                "Recall": float(rec),
                "AUC_ROC": float(auc),
                "Best_Threshold": float(best_threshold),
                "CV_AUC_Mean": float(cv_scores.mean()),
                "CV_AUC_Std": float(cv_scores.std())
            })
            
            joblib.dump(best, f"data/06_models/classification/{name}.pkl")
            
            print(f"[{name}] AUC={auc:.4f}, F1={f1:.4f}, Precision={prec:.4f}, Recall={rec:.4f}")
            print(f"[{name}] Best threshold: {best_threshold:.4f}")
        
        except Exception as e:
            print(f"‚ùå Error entrenando {name}: {str(e)}")
            continue
    
    results_df = pd.DataFrame(results)
    print("\n" + "="*60)
    print("RESULTADOS FINALES CLASIFICACI√ìN (ordenados por AUC):")
    print("="*60)
    print(results_df.sort_values("AUC_ROC", ascending=False).to_string(index=False))
    
    return results_df
